{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDhB4iPaDtKe"
      },
      "source": [
        "# Lista 7 - Atenção Neural\n",
        "\n",
        "Nesta lista exploraremos o mecanismo de atenção neural. Os mecanismos de atenção foram originalmente propostos como uma forma de incrementar os modelos recorrentes de tradução automática de textos, adicionando um mecanismo que permitiria o alinhamento entre os elementos do texto de origem e os elementos do texto de saída.\n",
        "\n",
        "Após as propostas iniciais, os mecanismos de atenção neural tiveram seu escopo expandido, assumindo um lugar de protagonismo em vários dos avanços que se sucederaam no NLP e no aprendizado de máquina, como um todo.\n",
        "\n",
        "Esses avanços posteriores serão discutidos nas próximas semanas, quando falarmos sobre a atenção neural no contexto dos Transformers. Nessa lista abordaremos a atenção neural sob uma ótica similiar àquela de sua proposição inicial, como um  mecanismo associado às redes neurais recorrentes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7s_9_vyFFsnx"
      },
      "outputs": [],
      "source": [
        "from pandas import read_csv\n",
        "import numpy as np\n",
        "from keras import Model\n",
        "from keras.layers import Layer\n",
        "import keras.backend as K\n",
        "from keras.layers import Input, Dense, SimpleRNN\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Sequential"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLADLVOgFLuu"
      },
      "source": [
        "## Dados\n",
        "\n",
        "Para explorarmos a ação dos mecanismos de atenção quando acoplados a redes recorrentes usaremos um problema simplificado: prever qual o próximo termo da [sequência de Fibonacci](https://en.wikipedia.org/wiki/Fibonacci_sequence) a partir de uma janela dos $t$ termos anteriores.\n",
        "\n",
        "Abaixo disponibilizamos o código, baseado [neste material](https://machinelearningmastery.com/adding-a-custom-attention-layer-to-recurrent-neural-network-in-keras/), que gera os números da sequência de Fibonacci na forma necessária para o treinamento e avaliação dos nossos modelos. O Código gera um conjunto de treinamento (`trainX` e `trainY`) e um conjunto de testes (`testX` e `testY`). Os exemplos de cada conjunto são janelas de $t$ termos sequenciais da sequência de Fobonacci com o termo seguinte como resposta. Os conjuntos são embaralhados e os números normalizados para facilitar o aprendizado da rede.\n",
        "Obs: Nos nossos experimentos, usaremos $t=20$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gma9R1SFlR_h"
      },
      "outputs": [],
      "source": [
        "def get_fib_seq(n, scale_data=True):\n",
        "    # Get the Fibonacci sequence\n",
        "    seq = np.zeros(n)\n",
        "    fib_n1 = 0.0\n",
        "    fib_n = 1.0\n",
        "    for i in range(n):\n",
        "            seq[i] = fib_n1 + fib_n\n",
        "            fib_n1 = fib_n\n",
        "            fib_n = seq[i]\n",
        "    scaler = []\n",
        "    if scale_data:\n",
        "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "        seq = np.reshape(seq, (n, 1))\n",
        "        seq = scaler.fit_transform(seq).flatten()\n",
        "    return seq, scaler\n",
        "\n",
        "def get_fib_XY(total_fib_numbers, time_steps, train_percent, scale_data=True):\n",
        "    dat, scaler = get_fib_seq(total_fib_numbers, scale_data)\n",
        "    Y_ind = np.arange(time_steps, len(dat), 1)\n",
        "    Y = dat[Y_ind]\n",
        "    rows_x = len(Y)\n",
        "    X = dat[0:rows_x]\n",
        "    for i in range(time_steps-1):\n",
        "        temp = dat[i+1:rows_x+i+1]\n",
        "        X = np.column_stack((X, temp))\n",
        "    # random permutation with fixed seed\n",
        "    rand = np.random.RandomState(seed=13)\n",
        "    idx = rand.permutation(rows_x)\n",
        "    split = int(train_percent*rows_x)\n",
        "    train_ind = idx[0:split]\n",
        "    test_ind = idx[split:]\n",
        "    trainX = X[train_ind]\n",
        "    trainY = Y[train_ind]\n",
        "    testX = X[test_ind]\n",
        "    testY = Y[test_ind]\n",
        "    trainX = np.reshape(trainX, (len(trainX), time_steps, 1))\n",
        "    testX = np.reshape(testX, (len(testX), time_steps, 1))\n",
        "    return trainX, trainY, testX, testY, scaler\n",
        "\n",
        "\n",
        "trainX, trainY, testX, testY, scaler  = get_fib_XY(1200, 20, 0.7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSclYn0wk0z1"
      },
      "source": [
        "# <font color='blue'>  Questão 1 </font>\n",
        "\n",
        "Explique, em poucas palavras, porque esse é um problema apropriado para ilustrar a aplicação de mecanismos de atenção às redes recorrentes. Se precisar, revise o conteúdo das últimas aulas, o texto de abertura dessa lista e a definição da sequência de de Fibonacci.\n",
        "\n",
        "_______________________________________________\n",
        "\n",
        "**<font color='red'> Sua resposta aqui </font>**\n",
        "\n",
        "A sequência de Fibonacci é dependente de termos anteriores na sequência, o que exige uma compreensão contextual para fazer previsões precisas. Os mecanismos de atenção permitem que a rede se concentre nos termos anteriores relevantes durante a previsão, capturando as relações de dependência na sequência."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u56JwMF0k4vR"
      },
      "source": [
        "## Rede Neural Recorrente sem Atenção\n",
        "\n",
        "Nosso objetivo final é comparar, nesssa tarefa simplificada, a performance de uma rede neural recorrente sem o mecanismo de atenção a uma rede neural recorrente com mecanismo de atenção.Vamos começar pela versão sem mecanismo de atenção.\n",
        "\n",
        "\n",
        "\n",
        "# <font color='blue'>  Questão 2 </font>\n",
        "\n",
        "Complete o código a seguir de de forma que o modelo implementado seja constituído de uma camada de Rede Neural recorrente básica com duas unidades escondidas, seguinda de uma camada densa com uma unidade. Para ambas as camadas use a Tangente Hibperbólica como função de ativação\n",
        "Obs: para a camada recorrente use a camada pré-implementada [Simple RNN](https://keras.io/api/layers/recurrent_layers/simple_rnn/), do Keras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azFpw67hnH07",
        "outputId": "c840df0c-c260-4836-8435-7dc4322b7395"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "826/826 - 1s - loss: 0.0014 - 923ms/epoch - 1ms/step\n",
            "Epoch 2/30\n",
            "826/826 - 1s - loss: 0.0012 - 657ms/epoch - 796us/step\n",
            "Epoch 3/30\n",
            "826/826 - 1s - loss: 9.6951e-04 - 670ms/epoch - 812us/step\n",
            "Epoch 4/30\n",
            "826/826 - 1s - loss: 7.9772e-04 - 676ms/epoch - 819us/step\n",
            "Epoch 5/30\n",
            "826/826 - 1s - loss: 6.4717e-04 - 659ms/epoch - 798us/step\n",
            "Epoch 6/30\n",
            "826/826 - 1s - loss: 5.1534e-04 - 660ms/epoch - 799us/step\n",
            "Epoch 7/30\n",
            "826/826 - 1s - loss: 3.9050e-04 - 661ms/epoch - 801us/step\n",
            "Epoch 8/30\n",
            "826/826 - 1s - loss: 2.9022e-04 - 667ms/epoch - 808us/step\n",
            "Epoch 9/30\n",
            "826/826 - 1s - loss: 2.3023e-04 - 683ms/epoch - 827us/step\n",
            "Epoch 10/30\n",
            "826/826 - 1s - loss: 1.9647e-04 - 664ms/epoch - 804us/step\n",
            "Epoch 11/30\n",
            "826/826 - 1s - loss: 1.6713e-04 - 652ms/epoch - 789us/step\n",
            "Epoch 12/30\n",
            "826/826 - 1s - loss: 1.4457e-04 - 668ms/epoch - 809us/step\n",
            "Epoch 13/30\n",
            "826/826 - 1s - loss: 1.2577e-04 - 693ms/epoch - 839us/step\n",
            "Epoch 14/30\n",
            "826/826 - 1s - loss: 1.1487e-04 - 649ms/epoch - 786us/step\n",
            "Epoch 15/30\n",
            "826/826 - 1s - loss: 1.0197e-04 - 647ms/epoch - 783us/step\n",
            "Epoch 16/30\n",
            "826/826 - 1s - loss: 9.3587e-05 - 679ms/epoch - 822us/step\n",
            "Epoch 17/30\n",
            "826/826 - 1s - loss: 8.6057e-05 - 666ms/epoch - 807us/step\n",
            "Epoch 18/30\n",
            "826/826 - 1s - loss: 8.2041e-05 - 663ms/epoch - 803us/step\n",
            "Epoch 19/30\n",
            "826/826 - 1s - loss: 6.8655e-05 - 665ms/epoch - 805us/step\n",
            "Epoch 20/30\n",
            "826/826 - 1s - loss: 7.0588e-05 - 647ms/epoch - 783us/step\n",
            "Epoch 21/30\n",
            "826/826 - 1s - loss: 6.3486e-05 - 672ms/epoch - 814us/step\n",
            "Epoch 22/30\n",
            "826/826 - 1s - loss: 5.4404e-05 - 665ms/epoch - 805us/step\n",
            "Epoch 23/30\n",
            "826/826 - 1s - loss: 5.0670e-05 - 670ms/epoch - 811us/step\n",
            "Epoch 24/30\n",
            "826/826 - 1s - loss: 4.9062e-05 - 662ms/epoch - 801us/step\n",
            "Epoch 25/30\n",
            "826/826 - 1s - loss: 4.0260e-05 - 665ms/epoch - 805us/step\n",
            "Epoch 26/30\n",
            "826/826 - 1s - loss: 4.1980e-05 - 654ms/epoch - 792us/step\n",
            "Epoch 27/30\n",
            "826/826 - 1s - loss: 3.6752e-05 - 677ms/epoch - 819us/step\n",
            "Epoch 28/30\n",
            "826/826 - 1s - loss: 3.2896e-05 - 656ms/epoch - 794us/step\n",
            "Epoch 29/30\n",
            "826/826 - 1s - loss: 3.5550e-05 - 655ms/epoch - 793us/step\n",
            "Epoch 30/30\n",
            "826/826 - 1s - loss: 2.9311e-05 - 673ms/epoch - 815us/step\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 2.0780e-05\n",
            "12/12 [==============================] - 0s 714us/step - loss: 1.5337e-05\n",
            "Train set MSE =  2.078007673844695e-05\n",
            "Test set MSE =  1.5337422155425884e-05\n"
          ]
        }
      ],
      "source": [
        "input_shape = (20, 1)\n",
        "model_vanilla = Sequential()\n",
        "#Seu começa aqui\n",
        "model_vanilla.add(SimpleRNN(2, activation='tanh', input_shape=input_shape))\n",
        "model_vanilla.add(Dense(1, activation='tanh'))\n",
        "#Seu código termina aqui\n",
        "\n",
        "model_vanilla.compile(loss='mse', optimizer='adam')\n",
        "\n",
        "model_vanilla.fit(trainX, trainY, epochs=30, batch_size=1, verbose=2)\n",
        "train_mse = model_vanilla.evaluate(trainX, trainY)\n",
        "test_mse = model_vanilla.evaluate(testX, testY)\n",
        "print(\"Train set MSE = \", train_mse)\n",
        "print(\"Test set MSE = \", test_mse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sja1JVlDxIWT"
      },
      "source": [
        "# Rede Neural recorrente com Atenção\n",
        "\n",
        "Agora vamos implementar a versão com atenção. Para tal implementaremos uma camada de atenção personalizada.\n",
        "\n",
        "Um dos mecanismos de atenção mais simples que existem pode ser definido através das seguintes equações:\n",
        "\n",
        "$E = tanh(X \\cdot W + b)$\n",
        "\n",
        "$\\alpha = softmax(E)$\n",
        "\n",
        "$X^\\prime = \\alpha X$\n",
        "\n",
        "Onde $X$ é a matriz que aglomera representações sequenciais de vários inputs, $\\alpha$ é a matriz com os pesos de atenção e $X^\\prime$ é a saída, uma nova representação para os inputs, pesada pelos fatores de atenção calculados. A matriz $W$ e o vetor $b$ são parâmetros internos da camada de atenção que serão otimizados no treinamento.\n",
        "# <font color='blue'>  Questão 3 </font>\n",
        "O código dado a seguir implementa uma camada de atenção personalizada seguindo o formato descrito até aqui, em seguida, constrói um modelo com essa camada de atenção, treina e testa nos dados. Complete as lacunas de maneira  a implementrar o mecanismo de atenção aqui descrito. Use as funções do módulo `keras.backend`, importado no início desta lista."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGE4yNQryKyg",
        "outputId": "9178d730-7c55-41ef-8361-cb7e7b319fa5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 20, 1)]           0         \n",
            "                                                                 \n",
            " simple_rnn_1 (SimpleRNN)    (None, 20, 2)             8         \n",
            "                                                                 \n",
            " attention (attention)       (None, 2)                 22        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 3         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 33 (132.00 Byte)\n",
            "Trainable params: 33 (132.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "826/826 - 1s - loss: 0.0015 - 1s/epoch - 1ms/step\n",
            "Epoch 2/30\n",
            "826/826 - 1s - loss: 0.0015 - 736ms/epoch - 891us/step\n",
            "Epoch 3/30\n",
            "826/826 - 1s - loss: 0.0014 - 727ms/epoch - 880us/step\n",
            "Epoch 4/30\n",
            "826/826 - 1s - loss: 0.0014 - 703ms/epoch - 851us/step\n",
            "Epoch 5/30\n",
            "826/826 - 1s - loss: 0.0014 - 711ms/epoch - 861us/step\n",
            "Epoch 6/30\n",
            "826/826 - 1s - loss: 0.0014 - 713ms/epoch - 864us/step\n",
            "Epoch 7/30\n",
            "826/826 - 1s - loss: 0.0014 - 708ms/epoch - 857us/step\n",
            "Epoch 8/30\n",
            "826/826 - 1s - loss: 0.0014 - 702ms/epoch - 850us/step\n",
            "Epoch 9/30\n",
            "826/826 - 1s - loss: 0.0014 - 700ms/epoch - 847us/step\n",
            "Epoch 10/30\n",
            "826/826 - 1s - loss: 0.0014 - 701ms/epoch - 849us/step\n",
            "Epoch 11/30\n",
            "826/826 - 1s - loss: 0.0014 - 701ms/epoch - 848us/step\n",
            "Epoch 12/30\n",
            "826/826 - 1s - loss: 0.0014 - 699ms/epoch - 846us/step\n",
            "Epoch 13/30\n",
            "826/826 - 1s - loss: 0.0014 - 706ms/epoch - 855us/step\n",
            "Epoch 14/30\n",
            "826/826 - 1s - loss: 0.0013 - 705ms/epoch - 853us/step\n",
            "Epoch 15/30\n",
            "826/826 - 1s - loss: 0.0013 - 705ms/epoch - 854us/step\n",
            "Epoch 16/30\n",
            "826/826 - 1s - loss: 0.0013 - 703ms/epoch - 851us/step\n",
            "Epoch 17/30\n",
            "826/826 - 1s - loss: 0.0013 - 718ms/epoch - 869us/step\n",
            "Epoch 18/30\n",
            "826/826 - 1s - loss: 0.0013 - 753ms/epoch - 912us/step\n",
            "Epoch 19/30\n",
            "826/826 - 1s - loss: 0.0013 - 714ms/epoch - 864us/step\n",
            "Epoch 20/30\n",
            "826/826 - 1s - loss: 0.0012 - 707ms/epoch - 856us/step\n",
            "Epoch 21/30\n",
            "826/826 - 1s - loss: 0.0012 - 781ms/epoch - 946us/step\n",
            "Epoch 22/30\n",
            "826/826 - 1s - loss: 0.0012 - 773ms/epoch - 936us/step\n",
            "Epoch 23/30\n",
            "826/826 - 1s - loss: 0.0012 - 728ms/epoch - 881us/step\n",
            "Epoch 24/30\n",
            "826/826 - 1s - loss: 0.0012 - 744ms/epoch - 901us/step\n",
            "Epoch 25/30\n",
            "826/826 - 1s - loss: 0.0011 - 736ms/epoch - 892us/step\n",
            "Epoch 26/30\n",
            "826/826 - 1s - loss: 0.0011 - 729ms/epoch - 883us/step\n",
            "Epoch 27/30\n",
            "826/826 - 1s - loss: 0.0011 - 737ms/epoch - 893us/step\n",
            "Epoch 28/30\n",
            "826/826 - 1s - loss: 0.0011 - 769ms/epoch - 931us/step\n",
            "Epoch 29/30\n",
            "826/826 - 1s - loss: 0.0010 - 719ms/epoch - 870us/step\n",
            "Epoch 30/30\n",
            "826/826 - 1s - loss: 9.8728e-04 - 710ms/epoch - 860us/step\n",
            "26/26 [==============================] - 0s 645us/step - loss: 9.4317e-04\n",
            "12/12 [==============================] - 0s 656us/step - loss: 8.3447e-04\n",
            "Train set MSE with attention =  0.0009431674261577427\n",
            "Test set MSE with attention =  0.000834465550724417\n"
          ]
        }
      ],
      "source": [
        "class attention(Layer):\n",
        "    def __init__(self,**kwargs):\n",
        "        super(attention,self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        self.W=self.add_weight(name='attention_weight', shape=(input_shape[-1],1),\n",
        "                               initializer='random_normal', trainable=True)\n",
        "        self.b=self.add_weight(name='attention_bias', shape=(input_shape[1],1),\n",
        "                               initializer='zeros', trainable=True)\n",
        "        super(attention, self).build(input_shape)\n",
        "\n",
        "    def call(self,x):\n",
        "        # Descomente e complete a linha a seguir:\n",
        "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
        "        # Remove dimension of size 1\n",
        "        e = K.squeeze(e, axis=-1)\n",
        "        # Compute the weights\n",
        "        # Descomente e complete a linha a seguir:\n",
        "        alpha = K.softmax(e)\n",
        "        # Reshape to tensorFlow format\n",
        "        alpha = K.expand_dims(alpha, axis=-1)\n",
        "        # Compute the context vector\n",
        "        context = x * alpha\n",
        "        context = K.sum(context, axis=1)\n",
        "        return context\n",
        "\n",
        "def create_RNN_with_attention(hidden_units, dense_units, input_shape, activation):\n",
        "    x=Input(shape=input_shape)\n",
        "    RNN_layer = SimpleRNN(hidden_units, return_sequences=True, activation=activation)(x)\n",
        "    attention_layer = attention()(RNN_layer)\n",
        "    outputs=Dense(dense_units, trainable=True, activation=activation)(attention_layer)\n",
        "    model=Model(x,outputs)\n",
        "    model.compile(loss='mse', optimizer='adam')\n",
        "    return model\n",
        "\n",
        "# Create the model with attention, train and evaluate\n",
        "model_attention = create_RNN_with_attention(hidden_units=2, dense_units=1,\n",
        "                                  input_shape=(20,1), activation='tanh')\n",
        "model_attention.summary()\n",
        "\n",
        "\n",
        "model_attention.fit(trainX, trainY, epochs=30, batch_size=1, verbose=2)\n",
        "\n",
        "# Evalute model\n",
        "train_mse_attn = model_attention.evaluate(trainX, trainY)\n",
        "test_mse_attn = model_attention.evaluate(testX, testY)\n",
        "\n",
        "# Print error\n",
        "print(\"Train set MSE with attention = \", train_mse_attn)\n",
        "print(\"Test set MSE with attention = \", test_mse_attn)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
